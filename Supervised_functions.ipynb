{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions file\n",
    "This is the file i have stored all my functions in and run in the supervised_learning_assignment notebook using %run.\n",
    "\n",
    "Author: Rients Dalstra <br>\n",
    "All functions are written by me, however during my bachelor thesis I also compared the performance of 5 different classifiers against each other and I have copied some of the functions from then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "import eli5\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function created to split and scale the data.\n",
    "def datasplit(X, y, size=0.8, scale=True):\n",
    "    \n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, y, train_size=size, random_state=42)\n",
    "\n",
    "    if scale==True:\n",
    "        xScaler = StandardScaler()\n",
    "        xScaler.fit(Xtrain)\n",
    "        Xtrain = xScaler.transform(Xtrain)\n",
    "        Xtest = xScaler.transform(Xtest)\n",
    "    \n",
    "    return Xtrain,Xtest,Ytrain,Ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functie used to perform and display permutation importance.\n",
    "def permutation_importance(X, y,input_model):\n",
    "\n",
    "    Xtrain, Xtest, Ytrain, Ytest = datasplit(X, y)\n",
    "    \n",
    "    model, params = input_model \n",
    "    model.fit(Xtrain,Ytrain)\n",
    "\n",
    "    perm = eli5.sklearn.PermutationImportance(model, random_state=42).fit(Xtrain, Ytrain)\n",
    "    display(eli5.show_weights(perm,feature_names = X.columns.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function created to obtain some metrics for the rfc model to see if it makes sense to use permutation importance\n",
    "def accuracy_scores_dct(X,y,k):\n",
    "\n",
    "    score={}    \n",
    "    Xtrain, Xtest, Ytrain, Ytest = datasplit(X, y)\n",
    "\n",
    "    model, params = dct_model()\n",
    "    model.fit(Xtrain,Ytrain)\n",
    "    y_pred = [i for i in model.predict(Xtest)]         \n",
    "    # accuracy: (tp + tn) / (p + n)\n",
    "    accuracy = metrics.accuracy_score(Ytest, y_pred)\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = metrics.precision_score(Ytest, y_pred)     \n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = metrics.recall_score(Ytest, y_pred)\n",
    "    # f1: 2 tp / (2 tp + fp + fn)\n",
    "    f1 = metrics.f1_score(Ytest, y_pred)\n",
    "    score['X'+k] = {'Accuracy':accuracy,'Precision':precision,'Recall':recall,'F1':f1}\n",
    "    display(pd.DataFrame.from_dict(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functie om te zien welke features het beste werken in vergelijking tot elkaar.\n",
    "def feature_importance(X, y,input_model): \n",
    "\n",
    "    Xtrain, Xtest, Ytrain, Ytest = datasplit(X, y)\n",
    "    \n",
    "    model, params = input_model \n",
    "    model.fit(Xtrain,Ytrain)\n",
    "    \n",
    "    feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "\n",
    "    # De grafiek plotten met nlargest beste features\n",
    "    plt.figure(figsize=(15,5))\n",
    "    ax = plt.gca()\n",
    "    feat_importances.nlargest(20).plot(kind='bar', color=['green','blue'],edgecolor='black',linewidth= 3, ax=ax)\n",
    "    plt.grid(visible=None, which='major', axis='y')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.title('The best Features')\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomizedSearch\n",
    "def train_RandomizedSearch(model, params, Xtrain, Ytrain, iter_nr=100, jobs=-1, scored='f1'):\n",
    "    randomizedsearch = RandomizedSearchCV(estimator = model, param_distributions = params, n_iter=iter_nr, n_jobs=jobs,scoring=scored)\n",
    "    randomizedsearch.fit(Xtrain,Ytrain)\n",
    "    return randomizedsearch.cv_results_[\"mean_test_score\"], randomizedsearch.best_params_, randomizedsearch.cv_results_[\"std_test_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to obtain best hyperparameters\n",
    "def hyperpara_check(X, y, input_model, iter_nr=100, scored='f1'): \n",
    "\n",
    "    Xtrain, Xtest, Ytrain, Ytest = datasplit(X, y)\n",
    "    \n",
    "    model, params = input_model \n",
    "    \n",
    "    meanscore, best_params, stdscore = train_RandomizedSearch(model, params, Xtrain, Ytrain, iter_nr=iter_nr, scored=scored)\n",
    "    \n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_scores(X,y, models_list=1):\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    if models_list == 1:\n",
    "        model_lijst = ['rfc','svm','mlpc','gbc','lr','dct','kmeans']\n",
    "    else:\n",
    "        model_lijst = ['rfc','lr','svm','gbc']\n",
    "    score={}    \n",
    "    for mod in model_lijst:\n",
    "            Xtrain, Xtest, Ytrain, Ytest = datasplit(X, y)\n",
    "            if mod == \"rfc\" :\n",
    "                model, params = rfc_model()\n",
    "            elif mod == \"gbc\":                              \n",
    "                model, params = gbc_model()\n",
    "            elif mod == \"mlpc\":\n",
    "                model, params = mlpc_model()\n",
    "            elif mod == \"lr\":\n",
    "                model, params = lr_model()\n",
    "            elif mod == \"svm\":\n",
    "                model, params = svm_model()\n",
    "            elif mod == \"dct\":\n",
    "                model, params = dct_model()\n",
    "            elif mod == \"kmeans\":\n",
    "                model, params = kmeans_model()\n",
    "                \n",
    "                \n",
    "            model.fit(Xtrain,Ytrain)\n",
    "            \n",
    "            #voorspelling voor elk model in de lijst\n",
    "            y_pred = [i for i in model.predict(Xtest)]        \n",
    "            \n",
    "            precision_curve, recall_curve, thresholds2 = metrics.precision_recall_curve(Ytest, model.predict_proba(Xtest)[:,1])\n",
    "                \n",
    "            #precision recall\n",
    "            ap = metrics.average_precision_score(Ytest, y_pred)\n",
    "            \n",
    "            #Grafiek maken\n",
    "            plt.plot(precision_curve, recall_curve, label='%s PR (area = %0.2f)' % (mod, ap))\n",
    "            \n",
    "            \n",
    "            plt.plot([0, 1], [0, 1],'r--')\n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "            plt.xlabel('Precision')\n",
    "            plt.ylabel('Recall')\n",
    "            plt.title('Precision Recall Curve')\n",
    "            plt.legend(loc=\"upper left\")\n",
    "            \n",
    "\n",
    "                \n",
    "            # accuracy: (tp + tn) / (p + n)\n",
    "            accuracy = metrics.accuracy_score(Ytest, y_pred)\n",
    "            # precision tp / (tp + fp)\n",
    "            precision = metrics.precision_score(Ytest, y_pred)     \n",
    "            # recall: tp / (tp + fn)\n",
    "            recall = metrics.recall_score(Ytest, y_pred)\n",
    "            # f1: 2 tp / (2 tp + fp + fn)\n",
    "            f1 = metrics.f1_score(Ytest, y_pred)\n",
    "            # kappa, lastig uit te leggen.\n",
    "            kappa = metrics.cohen_kappa_score(Ytest, y_pred)\n",
    "            # ROC AUC\n",
    "            auc = metrics.roc_auc_score(Ytest, y_pred)\n",
    "            \n",
    "            # confusion dataset\n",
    "            confusion = metrics.confusion_matrix(Ytest, y_pred)\n",
    "\n",
    "            score[mod] = {'Accuracy':accuracy,'Precision':precision,'Recall':recall,'F1':f1,'ROC AUC':auc,'Kappa':kappa}\n",
    "\n",
    "    plt.show()\n",
    "    return pd.DataFrame.from_dict(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_scores2(X,y, models_list=1):\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    if models_list == 1:\n",
    "        model_lijst = ['rfc','svm','mlpc','gbc','lr','dct','kmeans']\n",
    "    else:\n",
    "        model_lijst = ['rfc','lr','svm','gbc']\n",
    "    score={}    \n",
    "    for mod in model_lijst:\n",
    "            Xtrain, Xtest, Ytrain, Ytest = datasplit(X, y)\n",
    "            if mod == \"rfc\" :\n",
    "                model, params = rfc_model2()\n",
    "            elif mod == \"gbc\":                              \n",
    "                model, params = gbc_model2()\n",
    "            elif mod == \"mlpc\":\n",
    "                model, params = mlpc_model2()\n",
    "            elif mod == \"lr\":\n",
    "                model, params = lr_model2()\n",
    "            elif mod == \"svm\":\n",
    "                model, params = svm_model2()\n",
    "            elif mod == \"dct\":\n",
    "                model, params = dct_model()\n",
    "            elif mod == \"kmeans\":\n",
    "                model, params = kmeans_model2()\n",
    "                \n",
    "                \n",
    "            model.fit(Xtrain,Ytrain)\n",
    "            \n",
    "            #voorspelling voor elk model in de lijst\n",
    "            y_pred = [i for i in model.predict(Xtest)]        \n",
    "            \n",
    "            precision_curve, recall_curve, thresholds2 = metrics.precision_recall_curve(Ytest, model.predict_proba(Xtest)[:,1])\n",
    "                \n",
    "            #precision recall\n",
    "            ap = metrics.average_precision_score(Ytest, y_pred)\n",
    "            \n",
    "            #Grafiek maken\n",
    "            plt.plot(precision_curve, recall_curve, label='%s PR (area = %0.2f)' % (mod, ap))\n",
    "            \n",
    "            \n",
    "            plt.plot([0, 1], [0, 1],'r--')\n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "            plt.xlabel('Precision')\n",
    "            plt.ylabel('Recall')\n",
    "            plt.title('Precision Recall Curve')\n",
    "            plt.legend(loc=\"upper left\")\n",
    "            \n",
    "\n",
    "                \n",
    "            # accuracy: (tp + tn) / (p + n)\n",
    "            accuracy = metrics.accuracy_score(Ytest, y_pred)\n",
    "            # precision tp / (tp + fp)\n",
    "            precision = metrics.precision_score(Ytest, y_pred)     \n",
    "            # recall: tp / (tp + fn)\n",
    "            recall = metrics.recall_score(Ytest, y_pred)\n",
    "            # f1: 2 tp / (2 tp + fp + fn)\n",
    "            f1 = metrics.f1_score(Ytest, y_pred)\n",
    "            # kappa, lastig uit te leggen.\n",
    "            kappa = metrics.cohen_kappa_score(Ytest, y_pred)\n",
    "            # ROC AUC\n",
    "            auc = metrics.roc_auc_score(Ytest, y_pred)\n",
    "            \n",
    "            # confusion dataset\n",
    "            confusion = metrics.confusion_matrix(Ytest, y_pred)\n",
    "\n",
    "            score[mod] = {'Accuracy':accuracy,'Precision':precision,'Recall':recall,'F1':f1,'ROC AUC':auc,'Kappa':kappa}\n",
    "\n",
    "    plt.show()\n",
    "    return pd.DataFrame.from_dict(score)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6fb11f3da31993b6b771c1afe049802c8e31320ab3b51a45684f31f055e3b17e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('prog')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
